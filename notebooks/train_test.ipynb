{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beb801ca-fa2c-4077-baec-be5eff15735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from imblearn.pipeline import Pipeline\n",
    "# basic manipulation path tools\n",
    "import os\n",
    "from pathlib import Path\n",
    "notebook_path = Path().absolute()\n",
    "project_root = notebook_path.parent\n",
    "notebook_path = Path().absolute()\n",
    "project_root = notebook_path.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40bf9808-a9ba-4a4c-a028-7eb7f2c0e542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>class</th>\n",
       "      <th>subclass</th>\n",
       "      <th>full_class</th>\n",
       "      <th>recno</th>\n",
       "      <th>spt</th>\n",
       "      <th>metallicity_fe_h_1</th>\n",
       "      <th>visual_magnitude</th>\n",
       "      <th>effective_temperature_2</th>\n",
       "      <th>log_surface_gravity_2</th>\n",
       "      <th>radius</th>\n",
       "      <th>mass</th>\n",
       "      <th>luminosity</th>\n",
       "      <th>distance</th>\n",
       "      <th>lamost</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19850119_J025942.96+011122.1</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>F0</td>\n",
       "      <td>2251</td>\n",
       "      <td>kA5hA8mA9</td>\n",
       "      <td>0.220</td>\n",
       "      <td>12.872</td>\n",
       "      <td>7571.0</td>\n",
       "      <td>4.1632</td>\n",
       "      <td>1.820</td>\n",
       "      <td>1.760</td>\n",
       "      <td>9.80948</td>\n",
       "      <td>1171.0200</td>\n",
       "      <td>J025942.96+011122.1</td>\n",
       "      <td>1985-01-19 10:44:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19850119_J025947.35+020834.1</td>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "      <td>A6IV</td>\n",
       "      <td>2254</td>\n",
       "      <td>kA4hA9mA9</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>14.712</td>\n",
       "      <td>7137.0</td>\n",
       "      <td>3.8647</td>\n",
       "      <td>2.440</td>\n",
       "      <td>1.590</td>\n",
       "      <td>13.91443</td>\n",
       "      <td>3164.3701</td>\n",
       "      <td>J025947.35+020834.1</td>\n",
       "      <td>1985-01-19 11:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19850119_J030043.83+021812.4</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>A2V</td>\n",
       "      <td>2273</td>\n",
       "      <td>kA3hA5mA7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.446</td>\n",
       "      <td>6234.0</td>\n",
       "      <td>4.3243</td>\n",
       "      <td>1.254</td>\n",
       "      <td>1.210</td>\n",
       "      <td>2.13966</td>\n",
       "      <td>440.9360</td>\n",
       "      <td>J030043.83+021812.4</td>\n",
       "      <td>1985-01-19 10:44:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19850119_J030336.54+025459.5</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>A7V</td>\n",
       "      <td>2332</td>\n",
       "      <td>kA3hA6mA7</td>\n",
       "      <td>-0.467</td>\n",
       "      <td>12.935</td>\n",
       "      <td>7232.0</td>\n",
       "      <td>4.3071</td>\n",
       "      <td>1.480</td>\n",
       "      <td>1.620</td>\n",
       "      <td>5.39942</td>\n",
       "      <td>905.6660</td>\n",
       "      <td>J030336.54+025459.5</td>\n",
       "      <td>1985-01-19 10:44:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19861124_J025406.46+025537.7</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>A3V</td>\n",
       "      <td>2151</td>\n",
       "      <td>kA2hA4mA7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.013</td>\n",
       "      <td>6383.0</td>\n",
       "      <td>4.3332</td>\n",
       "      <td>1.277</td>\n",
       "      <td>1.280</td>\n",
       "      <td>2.43731</td>\n",
       "      <td>2151.9600</td>\n",
       "      <td>J025406.46+025537.7</td>\n",
       "      <td>1986-11-24 15:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21630</th>\n",
       "      <td>19830307_J145332.99-020651.5</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>A3IV</td>\n",
       "      <td>18473</td>\n",
       "      <td>kA2hA4mA7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.328</td>\n",
       "      <td>7039.0</td>\n",
       "      <td>3.4076</td>\n",
       "      <td>4.078</td>\n",
       "      <td>1.550</td>\n",
       "      <td>36.77406</td>\n",
       "      <td>4210.2002</td>\n",
       "      <td>J145332.99-020651.5</td>\n",
       "      <td>1983-03-07 19:31:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21631</th>\n",
       "      <td>19860518_J150420.63-012544.0</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>A7V</td>\n",
       "      <td>18498</td>\n",
       "      <td>kA3hA5mA7</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>11.774</td>\n",
       "      <td>7477.0</td>\n",
       "      <td>4.2316</td>\n",
       "      <td>1.663</td>\n",
       "      <td>1.720</td>\n",
       "      <td>7.78983</td>\n",
       "      <td>717.9860</td>\n",
       "      <td>J150420.63-012544.0</td>\n",
       "      <td>1986-05-18 16:41:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21632</th>\n",
       "      <td>19821001_J213339.08-082740.4</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>A7IV</td>\n",
       "      <td>20746</td>\n",
       "      <td>kA7hF1mF1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.689</td>\n",
       "      <td>7129.0</td>\n",
       "      <td>4.1760</td>\n",
       "      <td>1.705</td>\n",
       "      <td>1.590</td>\n",
       "      <td>6.76456</td>\n",
       "      <td>400.5130</td>\n",
       "      <td>J213339.08-082740.4</td>\n",
       "      <td>1982-10-01 13:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21633</th>\n",
       "      <td>19821008_J210850.48-082114.5</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>A7IV</td>\n",
       "      <td>20552</td>\n",
       "      <td>kA7hF0mF1</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>9.879</td>\n",
       "      <td>7330.0</td>\n",
       "      <td>4.1473</td>\n",
       "      <td>1.801</td>\n",
       "      <td>1.661</td>\n",
       "      <td>8.43666</td>\n",
       "      <td>311.7170</td>\n",
       "      <td>J210850.48-082114.5</td>\n",
       "      <td>1982-10-08 13:41:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21634</th>\n",
       "      <td>19821008_J212231.01-074710.8</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>F0</td>\n",
       "      <td>20638</td>\n",
       "      <td>kA7hF2mF1</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>10.669</td>\n",
       "      <td>7188.0</td>\n",
       "      <td>4.0805</td>\n",
       "      <td>1.915</td>\n",
       "      <td>1.610</td>\n",
       "      <td>8.82033</td>\n",
       "      <td>404.1070</td>\n",
       "      <td>J212231.01-074710.8</td>\n",
       "      <td>1982-10-08 12:44:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21635 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         identifier class  subclass full_class  recno  \\\n",
       "0      19850119_J025942.96+011122.1     F         0         F0   2251   \n",
       "1      19850119_J025947.35+020834.1     A         6       A6IV   2254   \n",
       "2      19850119_J030043.83+021812.4     A         2        A2V   2273   \n",
       "3      19850119_J030336.54+025459.5     A         7        A7V   2332   \n",
       "4      19861124_J025406.46+025537.7     A         3        A3V   2151   \n",
       "...                             ...   ...       ...        ...    ...   \n",
       "21630  19830307_J145332.99-020651.5     A         3       A3IV  18473   \n",
       "21631  19860518_J150420.63-012544.0     A         7        A7V  18498   \n",
       "21632  19821001_J213339.08-082740.4     A         7       A7IV  20746   \n",
       "21633  19821008_J210850.48-082114.5     A         7       A7IV  20552   \n",
       "21634  19821008_J212231.01-074710.8     F         0         F0  20638   \n",
       "\n",
       "             spt  metallicity_fe_h_1  visual_magnitude  \\\n",
       "0      kA5hA8mA9               0.220            12.872   \n",
       "1      kA4hA9mA9              -0.367            14.712   \n",
       "2      kA3hA5mA7                 NaN            12.446   \n",
       "3      kA3hA6mA7              -0.467            12.935   \n",
       "4      kA2hA4mA7                 NaN            16.013   \n",
       "...          ...                 ...               ...   \n",
       "21630  kA2hA4mA7                 NaN            14.328   \n",
       "21631  kA3hA5mA7              -0.343            11.774   \n",
       "21632  kA7hF1mF1                 NaN            10.689   \n",
       "21633  kA7hF0mF1              -0.043             9.879   \n",
       "21634  kA7hF2mF1              -0.026            10.669   \n",
       "\n",
       "       effective_temperature_2  log_surface_gravity_2  radius   mass  \\\n",
       "0                       7571.0                 4.1632   1.820  1.760   \n",
       "1                       7137.0                 3.8647   2.440  1.590   \n",
       "2                       6234.0                 4.3243   1.254  1.210   \n",
       "3                       7232.0                 4.3071   1.480  1.620   \n",
       "4                       6383.0                 4.3332   1.277  1.280   \n",
       "...                        ...                    ...     ...    ...   \n",
       "21630                   7039.0                 3.4076   4.078  1.550   \n",
       "21631                   7477.0                 4.2316   1.663  1.720   \n",
       "21632                   7129.0                 4.1760   1.705  1.590   \n",
       "21633                   7330.0                 4.1473   1.801  1.661   \n",
       "21634                   7188.0                 4.0805   1.915  1.610   \n",
       "\n",
       "       luminosity   distance               lamost                 date  \n",
       "0         9.80948  1171.0200  J025942.96+011122.1  1985-01-19 10:44:00  \n",
       "1        13.91443  3164.3701  J025947.35+020834.1  1985-01-19 11:57:00  \n",
       "2         2.13966   440.9360  J030043.83+021812.4  1985-01-19 10:44:00  \n",
       "3         5.39942   905.6660  J030336.54+025459.5  1985-01-19 10:44:00  \n",
       "4         2.43731  2151.9600  J025406.46+025537.7  1986-11-24 15:16:00  \n",
       "...           ...        ...                  ...                  ...  \n",
       "21630    36.77406  4210.2002  J145332.99-020651.5  1983-03-07 19:31:00  \n",
       "21631     7.78983   717.9860  J150420.63-012544.0  1986-05-18 16:41:00  \n",
       "21632     6.76456   400.5130  J213339.08-082740.4  1982-10-01 13:53:00  \n",
       "21633     8.43666   311.7170  J210850.48-082114.5  1982-10-08 13:41:00  \n",
       "21634     8.82033   404.1070  J212231.01-074710.8  1982-10-08 12:44:00  \n",
       "\n",
       "[21635 rows x 16 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv('data_lakehouse/gold/starG.csv')\n",
    "df = pd.read_csv(project_root/'data'/'gold'/'starG.csv')\n",
    "#df1 = df.drop(['FeH2', 'Teff2', 'logg2', 'Rad', 'Mass'], axis=1)\n",
    "df1 = df.drop(['effective_temperature_1', 'log_surface_gravity_1', 'metallicity_fe_h_2', 'luminosity_class']\n",
    ", axis=1)\n",
    "#df1['Miss'] = df1['Teff1'].isnull().astype(int)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "359be3f3-b902-44ef-8b06-e02a919a879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aqui vou montar uma Pipeline para o dataframes que estão no formato do df1, ou seja,\n",
    "## dataframes sem as colunas 'effective_temperature_1', 'log_surface_gravity_1', 'metallicity_fe_h_2', 'luminosity_class'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33836c3f-b9db-4c71-812a-9333690291e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df1.dropna()\n",
    "df_filtered = df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89b6680-96fb-46e9-9861-284ec95966e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import KNNImputer\n",
    "\n",
    "# # Define KNN imputer\n",
    "# knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# # Copy df1 for imputation\n",
    "# df_imputed = df1.copy()\n",
    "\n",
    "# # Perform KNN imputation only for 'Teff2' column\n",
    "# df_imputed['Teff2'] = knn_imputer.fit_transform(df_imputed[['Teff2']])\n",
    "\n",
    "# # Add 'Miss' column indicating if the value was imputed\n",
    "# df_imputed['Teff2_Miss'] = df1['Teff2'].isna().astype(int)\n",
    "\n",
    "# # Assign the result to df_filtered\n",
    "# df_filtered = df_imputed\n",
    "# df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "219495e6-6b71-4663-a2cf-67d6e42cb62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Enhanced Evaluation ===\n",
      "Validation Accuracy: 0.7149\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.78      0.38      0.51       448\n",
      "           F       0.70      0.93      0.80       699\n",
      "\n",
      "    accuracy                           0.71      1147\n",
      "   macro avg       0.74      0.65      0.65      1147\n",
      "weighted avg       0.73      0.71      0.69      1147\n",
      "\n",
      "\n",
      "Confusion Matrix (True vs Predicted):\n",
      "     A    F\n",
      "A  170  278\n",
      "F   49  650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitry/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration\n",
    "TARGET_COLUMN = ['class']\n",
    "FEATURE_COLUMNS = ['effective_temperature_2', 'radius', 'mass']\n",
    "\n",
    "# Prepare data\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_filtered[TARGET_COLUMN])\n",
    "X = df_filtered[FEATURE_COLUMNS]\n",
    "\n",
    "# Split data (stratified)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, \n",
    "                                                 stratify=y, random_state=42)\n",
    "\n",
    "# 1. Handle Class Imbalance automatically with SMOTE\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 2. Train Model with Balanced Data and Optimized Parameters\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=20,\n",
    "    max_depth=1,\n",
    "    #min_samples_split=int(1e2),\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42,\n",
    "    #n_jobs=-1\n",
    ")\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# 3. Enhanced Evaluation\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(\"\\n=== Enhanced Evaluation ===\")\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, \n",
    "                          target_names=label_encoder.classes_,\n",
    "                          zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix (True vs Predicted):\")\n",
    "print(pd.DataFrame(confusion_matrix(y_val, y_pred),\n",
    "                  index=label_encoder.classes_,\n",
    "                  columns=label_encoder.classes_))\n",
    "\n",
    "# 4. Predictions with Confidence\n",
    "#probs = model.predict_proba(X_val)\n",
    "#results = df_filtered.iloc[X_val.index].copy()\n",
    "#results['Predicted'] = label_encoder.inverse_transform(y_pred)\n",
    "#results['Confidence'] = np.max(probs, axis=1)\n",
    "\n",
    "#print(\"\\nSample Predictions with Confidence Scores:\")\n",
    "#print(results[[TARGET_COLUMN] + FEATURE_COLUMNS + ['Predicted', 'Confidence']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0293981e-cbee-425b-8e61-6c62b22afc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7149\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.78      0.38      0.51       448\n",
      "           F       0.70      0.93      0.80       699\n",
      "\n",
      "    accuracy                           0.71      1147\n",
      "   macro avg       0.74      0.65      0.65      1147\n",
      "weighted avg       0.73      0.71      0.69      1147\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "     A    F\n",
      "A  170  278\n",
      "F   49  650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitry/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "## Cópia da célula acima utilizando Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "TARGET_COLUMN = ['class']\n",
    "FEATURE_COLUMNS = ['effective_temperature_2', 'radius', 'mass']\n",
    "\n",
    "# Prepare data\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_filtered[TARGET_COLUMN])\n",
    "X = df_filtered[FEATURE_COLUMNS]\n",
    "\n",
    "# Split data (stratified)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, \n",
    "                                                 stratify=y, random_state=42)\n",
    "\n",
    "# Train model in Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(sampling_strategy='not majority', random_state=42)),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        n_estimators=20,\n",
    "        max_depth=1,\n",
    "        class_weight='balanced_subsample',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit model in Pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "print(\"Validation Accuracy:\", round(accuracy_score(y_val, y_pred), 4))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=label_encoder.classes_, zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(y_val, y_pred),\n",
    "                   index=label_encoder.classes_,\n",
    "                   columns=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96a86153-6ffe-4220-a749-8b13f6b4a264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "\n",
      "=== Gradient Boosted Model Evaluation ===\n",
      "Validation Accuracy: 0.6757\n",
      "\n",
      "Best Hyperparameters Found:\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 2}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.60      0.53      0.56       448\n",
      "           F       0.72      0.77      0.74       699\n",
      "\n",
      "    accuracy                           0.68      1147\n",
      "   macro avg       0.66      0.65      0.65      1147\n",
      "weighted avg       0.67      0.68      0.67      1147\n",
      "\n",
      "\n",
      "Confusion Matrix (True vs Predicted):\n",
      "     A    F\n",
      "A  237  211\n",
      "F  161  538\n"
     ]
    }
   ],
   "source": [
    "# New Cell: Hyperparameter Optimization using Gradient Boosting and Gradient Descent\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 1. Prepare model\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# 2. Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [1,2,3,4,5],\n",
    "    'learning_rate': [0.01, 1e-2, 1e-3],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# 3. Set up Grid Search with Cross-Validation\n",
    "grid_search = GridSearchCV(\n",
    "    gb_model,\n",
    "    param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. Fit on SMOTE-resampled training data\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# 5. Best model from grid search\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "\n",
    "# 6. Evaluation\n",
    "y_pred_gb = best_gb_model.predict(X_val)\n",
    "\n",
    "print(\"\\n=== Gradient Boosted Model Evaluation ===\")\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_pred_gb):.4f}\")\n",
    "\n",
    "print(\"\\nBest Hyperparameters Found:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred_gb, \n",
    "                          target_names=label_encoder.classes_,\n",
    "                          zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix (True vs Predicted):\")\n",
    "print(pd.DataFrame(confusion_matrix(y_val, y_pred_gb),\n",
    "                  index=label_encoder.classes_,\n",
    "                  columns=label_encoder.classes_))\n",
    "\n",
    "#print(\"\\nBest Hyperparameters Found:\")\n",
    "#print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a15c9262-6bd4-439a-96e7-d77d15b41a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Category Counts for 'sub_class' ===\n",
      "subclass\n",
      "0    6897\n",
      "7    2814\n",
      "5     780\n",
      "6     672\n",
      "2     129\n",
      "3      49\n",
      "8      46\n",
      "9      45\n",
      "1      34\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Remaining Classes After Filtering ===\n",
      "subclass\n",
      "0    6897\n",
      "7    2814\n",
      "5     780\n",
      "6     672\n",
      "2     129\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Count and display the number of entries per category\n",
    "print(\"\\n=== Category Counts for 'sub_class' ===\")\n",
    "print(df_clean['subclass'].value_counts())\n",
    "\n",
    "# 2. Define minimum count threshold (customizable)\n",
    "MIN_COUNT = 100  # Modify this value as needed\n",
    "\n",
    "# 3. Filter out classes with fewer entries than MIN_COUNT\n",
    "valid_classes = df_clean['subclass'].dropna().value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= MIN_COUNT].index\n",
    "\n",
    "df_filtered = df_clean[df_clean['subclass'].isin(valid_classes)].reset_index(drop=True).dropna()\n",
    "\n",
    "print(\"\\n=== Remaining Classes After Filtering ===\")\n",
    "print(df_filtered['subclass'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a55c0fc-0092-4594-9726-0d09d37b4ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4838\n",
      "\n",
      "Confusion Matrix (True vs Predicted):\n",
      "     0    2    5  6    7\n",
      "0  936  109   31  0  993\n",
      "2    3   27    7  0    2\n",
      "5   18    7  134  0   75\n",
      "6   26    8   95  0   73\n",
      "7  184   12  105  1  542\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration\n",
    "TARGET_COLUMN = 'subclass'\n",
    "FEATURE_COLUMNS = ['effective_temperature_2', 'visual_magnitude', 'mass', 'radius']\n",
    "\n",
    "# Prepare feature data\n",
    "X = df_filtered[FEATURE_COLUMNS]\n",
    "\n",
    "# Prepare target data\n",
    "y = df_filtered[TARGET_COLUMN]\n",
    "\n",
    "# Split data (stratified)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Handle Class Imbalance with SMOTE\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 2. Train Model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# 3. Evaluation\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Print Accuracy and Confusion Matrix\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "print(\"\\nConfusion Matrix (True vs Predicted):\")\n",
    "print(pd.DataFrame(\n",
    "    confusion_matrix(y_val, y_pred),\n",
    "    index=np.unique(y),\n",
    "    columns=np.unique(y)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc749f65-c473-4437-82ed-a172055aa1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>class</th>\n",
       "      <th>subclass</th>\n",
       "      <th>type</th>\n",
       "      <th>full_class</th>\n",
       "      <th>recno</th>\n",
       "      <th>spt</th>\n",
       "      <th>metallicity_fe_h_1</th>\n",
       "      <th>visual_magnitude</th>\n",
       "      <th>effective_temperature_2</th>\n",
       "      <th>log_surface_gravity_2</th>\n",
       "      <th>radius</th>\n",
       "      <th>mass</th>\n",
       "      <th>luminosity</th>\n",
       "      <th>distance</th>\n",
       "      <th>lamost</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19850119_J025942.96+011122.1</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>F0</td>\n",
       "      <td>F0</td>\n",
       "      <td>2251</td>\n",
       "      <td>kA5hA8mA9</td>\n",
       "      <td>0.220</td>\n",
       "      <td>12.872</td>\n",
       "      <td>7571.0</td>\n",
       "      <td>4.1632</td>\n",
       "      <td>1.820</td>\n",
       "      <td>1.760</td>\n",
       "      <td>9.80948</td>\n",
       "      <td>1171.0200</td>\n",
       "      <td>J025942.96+011122.1</td>\n",
       "      <td>1985-01-19 10:44:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19850119_J025947.35+020834.1</td>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "      <td>A6</td>\n",
       "      <td>A6IV</td>\n",
       "      <td>2254</td>\n",
       "      <td>kA4hA9mA9</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>14.712</td>\n",
       "      <td>7137.0</td>\n",
       "      <td>3.8647</td>\n",
       "      <td>2.440</td>\n",
       "      <td>1.590</td>\n",
       "      <td>13.91443</td>\n",
       "      <td>3164.3701</td>\n",
       "      <td>J025947.35+020834.1</td>\n",
       "      <td>1985-01-19 11:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19850119_J030043.83+021812.4</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>A2</td>\n",
       "      <td>A2V</td>\n",
       "      <td>2273</td>\n",
       "      <td>kA3hA5mA7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.446</td>\n",
       "      <td>6234.0</td>\n",
       "      <td>4.3243</td>\n",
       "      <td>1.254</td>\n",
       "      <td>1.210</td>\n",
       "      <td>2.13966</td>\n",
       "      <td>440.9360</td>\n",
       "      <td>J030043.83+021812.4</td>\n",
       "      <td>1985-01-19 10:44:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19850119_J030336.54+025459.5</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>A7</td>\n",
       "      <td>A7V</td>\n",
       "      <td>2332</td>\n",
       "      <td>kA3hA6mA7</td>\n",
       "      <td>-0.467</td>\n",
       "      <td>12.935</td>\n",
       "      <td>7232.0</td>\n",
       "      <td>4.3071</td>\n",
       "      <td>1.480</td>\n",
       "      <td>1.620</td>\n",
       "      <td>5.39942</td>\n",
       "      <td>905.6660</td>\n",
       "      <td>J030336.54+025459.5</td>\n",
       "      <td>1985-01-19 10:44:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19861124_J025406.46+025537.7</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3V</td>\n",
       "      <td>2151</td>\n",
       "      <td>kA2hA4mA7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.013</td>\n",
       "      <td>6383.0</td>\n",
       "      <td>4.3332</td>\n",
       "      <td>1.277</td>\n",
       "      <td>1.280</td>\n",
       "      <td>2.43731</td>\n",
       "      <td>2151.9600</td>\n",
       "      <td>J025406.46+025537.7</td>\n",
       "      <td>1986-11-24 15:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21630</th>\n",
       "      <td>19830307_J145332.99-020651.5</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>A3</td>\n",
       "      <td>A3IV</td>\n",
       "      <td>18473</td>\n",
       "      <td>kA2hA4mA7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.328</td>\n",
       "      <td>7039.0</td>\n",
       "      <td>3.4076</td>\n",
       "      <td>4.078</td>\n",
       "      <td>1.550</td>\n",
       "      <td>36.77406</td>\n",
       "      <td>4210.2002</td>\n",
       "      <td>J145332.99-020651.5</td>\n",
       "      <td>1983-03-07 19:31:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21631</th>\n",
       "      <td>19860518_J150420.63-012544.0</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>A7</td>\n",
       "      <td>A7V</td>\n",
       "      <td>18498</td>\n",
       "      <td>kA3hA5mA7</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>11.774</td>\n",
       "      <td>7477.0</td>\n",
       "      <td>4.2316</td>\n",
       "      <td>1.663</td>\n",
       "      <td>1.720</td>\n",
       "      <td>7.78983</td>\n",
       "      <td>717.9860</td>\n",
       "      <td>J150420.63-012544.0</td>\n",
       "      <td>1986-05-18 16:41:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21632</th>\n",
       "      <td>19821001_J213339.08-082740.4</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>A7</td>\n",
       "      <td>A7IV</td>\n",
       "      <td>20746</td>\n",
       "      <td>kA7hF1mF1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.689</td>\n",
       "      <td>7129.0</td>\n",
       "      <td>4.1760</td>\n",
       "      <td>1.705</td>\n",
       "      <td>1.590</td>\n",
       "      <td>6.76456</td>\n",
       "      <td>400.5130</td>\n",
       "      <td>J213339.08-082740.4</td>\n",
       "      <td>1982-10-01 13:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21633</th>\n",
       "      <td>19821008_J210850.48-082114.5</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>A7</td>\n",
       "      <td>A7IV</td>\n",
       "      <td>20552</td>\n",
       "      <td>kA7hF0mF1</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>9.879</td>\n",
       "      <td>7330.0</td>\n",
       "      <td>4.1473</td>\n",
       "      <td>1.801</td>\n",
       "      <td>1.661</td>\n",
       "      <td>8.43666</td>\n",
       "      <td>311.7170</td>\n",
       "      <td>J210850.48-082114.5</td>\n",
       "      <td>1982-10-08 13:41:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21634</th>\n",
       "      <td>19821008_J212231.01-074710.8</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>F0</td>\n",
       "      <td>F0</td>\n",
       "      <td>20638</td>\n",
       "      <td>kA7hF2mF1</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>10.669</td>\n",
       "      <td>7188.0</td>\n",
       "      <td>4.0805</td>\n",
       "      <td>1.915</td>\n",
       "      <td>1.610</td>\n",
       "      <td>8.82033</td>\n",
       "      <td>404.1070</td>\n",
       "      <td>J212231.01-074710.8</td>\n",
       "      <td>1982-10-08 12:44:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21635 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         identifier class  subclass type full_class  recno  \\\n",
       "0      19850119_J025942.96+011122.1     F         0   F0         F0   2251   \n",
       "1      19850119_J025947.35+020834.1     A         6   A6       A6IV   2254   \n",
       "2      19850119_J030043.83+021812.4     A         2   A2        A2V   2273   \n",
       "3      19850119_J030336.54+025459.5     A         7   A7        A7V   2332   \n",
       "4      19861124_J025406.46+025537.7     A         3   A3        A3V   2151   \n",
       "...                             ...   ...       ...  ...        ...    ...   \n",
       "21630  19830307_J145332.99-020651.5     A         3   A3       A3IV  18473   \n",
       "21631  19860518_J150420.63-012544.0     A         7   A7        A7V  18498   \n",
       "21632  19821001_J213339.08-082740.4     A         7   A7       A7IV  20746   \n",
       "21633  19821008_J210850.48-082114.5     A         7   A7       A7IV  20552   \n",
       "21634  19821008_J212231.01-074710.8     F         0   F0         F0  20638   \n",
       "\n",
       "             spt  metallicity_fe_h_1  visual_magnitude  \\\n",
       "0      kA5hA8mA9               0.220            12.872   \n",
       "1      kA4hA9mA9              -0.367            14.712   \n",
       "2      kA3hA5mA7                 NaN            12.446   \n",
       "3      kA3hA6mA7              -0.467            12.935   \n",
       "4      kA2hA4mA7                 NaN            16.013   \n",
       "...          ...                 ...               ...   \n",
       "21630  kA2hA4mA7                 NaN            14.328   \n",
       "21631  kA3hA5mA7              -0.343            11.774   \n",
       "21632  kA7hF1mF1                 NaN            10.689   \n",
       "21633  kA7hF0mF1              -0.043             9.879   \n",
       "21634  kA7hF2mF1              -0.026            10.669   \n",
       "\n",
       "       effective_temperature_2  log_surface_gravity_2  radius   mass  \\\n",
       "0                       7571.0                 4.1632   1.820  1.760   \n",
       "1                       7137.0                 3.8647   2.440  1.590   \n",
       "2                       6234.0                 4.3243   1.254  1.210   \n",
       "3                       7232.0                 4.3071   1.480  1.620   \n",
       "4                       6383.0                 4.3332   1.277  1.280   \n",
       "...                        ...                    ...     ...    ...   \n",
       "21630                   7039.0                 3.4076   4.078  1.550   \n",
       "21631                   7477.0                 4.2316   1.663  1.720   \n",
       "21632                   7129.0                 4.1760   1.705  1.590   \n",
       "21633                   7330.0                 4.1473   1.801  1.661   \n",
       "21634                   7188.0                 4.0805   1.915  1.610   \n",
       "\n",
       "       luminosity   distance               lamost                 date  \n",
       "0         9.80948  1171.0200  J025942.96+011122.1  1985-01-19 10:44:00  \n",
       "1        13.91443  3164.3701  J025947.35+020834.1  1985-01-19 11:57:00  \n",
       "2         2.13966   440.9360  J030043.83+021812.4  1985-01-19 10:44:00  \n",
       "3         5.39942   905.6660  J030336.54+025459.5  1985-01-19 10:44:00  \n",
       "4         2.43731  2151.9600  J025406.46+025537.7  1986-11-24 15:16:00  \n",
       "...           ...        ...                  ...                  ...  \n",
       "21630    36.77406  4210.2002  J145332.99-020651.5  1983-03-07 19:31:00  \n",
       "21631     7.78983   717.9860  J150420.63-012544.0  1986-05-18 16:41:00  \n",
       "21632     6.76456   400.5130  J213339.08-082740.4  1982-10-01 13:53:00  \n",
       "21633     8.43666   311.7170  J210850.48-082114.5  1982-10-08 13:41:00  \n",
       "21634     8.82033   404.1070  J212231.01-074710.8  1982-10-08 12:44:00  \n",
       "\n",
       "[21635 rows x 17 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.insert(\n",
    "    df1.columns.get_loc('subclass') + 1,\n",
    "    'type',\n",
    "    df1['class'].astype(str) + df1['subclass'].astype(str)\n",
    ")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63e2ce90-12bb-4143-852d-94de229f1756",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m y = df1.dropna()[TARGET_COLUMN]\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Split data (stratified)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m X_train, X_val, y_train, y_val = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 1. Handle Class Imbalance with SMOTE\u001b[39;00m\n\u001b[32m     24\u001b[39m smote = SMOTE(sampling_strategy=\u001b[33m'\u001b[39m\u001b[33mnot majority\u001b[39m\u001b[33m'\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/model_selection/_split.py:2872\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2868\u001b[39m         CVClass = ShuffleSplit\n\u001b[32m   2870\u001b[39m     cv = CVClass(test_size=n_test, train_size=n_train, random_state=random_state)\n\u001b[32m-> \u001b[39m\u001b[32m2872\u001b[39m     train, test = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2874\u001b[39m train, test = ensure_common_namespace_device(arrays[\u001b[32m0\u001b[39m], train, test)\n\u001b[32m   2876\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   2877\u001b[39m     chain.from_iterable(\n\u001b[32m   2878\u001b[39m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[32m   2879\u001b[39m     )\n\u001b[32m   2880\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/model_selection/_split.py:1909\u001b[39m, in \u001b[36mBaseShuffleSplit.split\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   1879\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[32m   1880\u001b[39m \n\u001b[32m   1881\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1906\u001b[39m \u001b[33;03mto an integer.\u001b[39;00m\n\u001b[32m   1907\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1908\u001b[39m X, y, groups = indexable(X, y, groups)\n\u001b[32m-> \u001b[39m\u001b[32m1909\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/model_selection/_split.py:2318\u001b[39m, in \u001b[36mStratifiedShuffleSplit._iter_indices\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   2316\u001b[39m class_counts = np.bincount(y_indices)\n\u001b[32m   2317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.min(class_counts) < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2318\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2319\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe least populated class in y has only 1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2320\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m member, which is too few. The minimum\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m number of groups for any class cannot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m be less than 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2323\u001b[39m     )\n\u001b[32m   2325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train < n_classes:\n\u001b[32m   2326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m should be greater or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m % (n_train, n_classes)\n\u001b[32m   2329\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration\n",
    "TARGET_COLUMN = 'full_class'\n",
    "FEATURE_COLUMNS = ['effective_temperature_2', 'visual_magnitude', 'mass', 'radius']\n",
    "\n",
    "# Prepare feature data\n",
    "X = df1.dropna()[FEATURE_COLUMNS]\n",
    "\n",
    "# Prepare target data\n",
    "y = df1.dropna()[TARGET_COLUMN]\n",
    "\n",
    "# Split data (stratified)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Handle Class Imbalance with SMOTE\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 2. Train Model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=50,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# 3. Evaluation\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Print Accuracy and Confusion Matrix\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "print(\"\\nConfusion Matrix (True vs Predicted):\")\n",
    "print(pd.DataFrame(\n",
    "    confusion_matrix(y_val, y_pred),\n",
    "    index=np.unique(y),\n",
    "    columns=np.unique(y)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6f9bb7c-686d-4e7c-8e17-00582e3d175a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4493\n",
      "Confusion Matrix:\n",
      "    A0   A1  A2  A3  A5  A6   A7  A8  A9    F0  F2\n",
      "A0   0    0   0   0   0   0    0   0   0     1   0\n",
      "A1   0  195  82   1  63  38   54   0   0    42   0\n",
      "A2   0  121  70   7  22  17   31   0   0    19   1\n",
      "A3   0   12   9   2   4   3   19   1   0    23   0\n",
      "A5   0   87  27   2  82  43  118   0   0   177   0\n",
      "A6   0   57  25   1  48  55  112   0   0   149   1\n",
      "A7   0   36  14   1  52  52  473   0   3   805   0\n",
      "A8   0    0   0   0   1   0    3   0   0    15   3\n",
      "A9   0    0   0   0   0   2    5   0   1    16   1\n",
      "F0   0   11   3   1  53  28  440   1   1  1545   7\n",
      "F2   0    0   1   1   0   0    2   0   0    11  13\n"
     ]
    }
   ],
   "source": [
    "## Coloncando a coluna \"type\" e treinando o modelo em uma pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# === 1. Define transformer to create 'type' column ===\n",
    "def create_type_column(df):\n",
    "    df = df.copy()\n",
    "    df['type'] = df['class'].astype(str) + df['subclass'].astype(str)\n",
    "    return df\n",
    "\n",
    "create_type = FunctionTransformer(create_type_column)\n",
    "\n",
    "# === 2. Select features and target ===\n",
    "FEATURE_COLUMNS = ['effective_temperature_2', 'visual_magnitude', 'mass', 'radius']\n",
    "TARGET_COLUMN = 'type'\n",
    "\n",
    "# === 3. Define the preprocessing pipeline ===\n",
    "feature_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('features', feature_pipeline, FEATURE_COLUMNS)\n",
    "])\n",
    "\n",
    "# === 4. Full pipeline including SMOTE and classifier ===\n",
    "model_pipeline = ImbPipeline([\n",
    "    ('create_type', create_type),  # Add 'type' column\n",
    "    ('preprocessor', preprocessor),  # Impute features\n",
    "    ('to_numpy', FunctionTransformer(lambda df: df.to_numpy(), validate=False)),  # Convert DataFrame to NumPy\n",
    "    # The next steps will be manually handled outside pipeline due to train/test split\n",
    "])\n",
    "\n",
    "# === 5. Apply transformations ===\n",
    "#df1 = ...  # your original DataFrame\n",
    "df1 = df1.dropna(subset=FEATURE_COLUMNS + ['class', 'subclass'])  # Drop rows with missing values\n",
    "\n",
    "# Apply 'type' column creation\n",
    "df_transformed = create_type.transform(df1)\n",
    "X = df_transformed[FEATURE_COLUMNS]\n",
    "y = df_transformed[TARGET_COLUMN]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.3, stratify=y_encoded, random_state=42)\n",
    "\n",
    "# Apply SMOTE\n",
    "#smote = SMOTE(sampling_strategy='not majority', random_state=42,k_neighbors=1)\n",
    "#X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=50,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_val)\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(y_val, y_pred),\n",
    "                   index=le.classes_, columns=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b36390cc-b081-41cc-a493-f03323df572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4493\n",
      "\n",
      "Confusion Matrix:\n",
      "    A0   A1  A2  A3  A5  A6   A7  A8  A9    F0  F2\n",
      "A0   0    0   0   0   0   0    0   0   0     1   0\n",
      "A1   0  195  82   1  63  38   54   0   0    42   0\n",
      "A2   0  121  70   7  22  17   31   0   0    19   1\n",
      "A3   0   12   9   2   4   3   19   1   0    23   0\n",
      "A5   0   87  27   2  82  43  118   0   0   177   0\n",
      "A6   0   57  25   1  48  55  112   0   0   149   1\n",
      "A7   0   36  14   1  52  52  473   0   3   805   0\n",
      "A8   0    0   0   0   1   0    3   0   0    15   3\n",
      "A9   0    0   0   0   0   2    5   0   1    16   1\n",
      "F0   0   11   3   1  53  28  440   1   1  1545   7\n",
      "F2   0    0   1   1   0   0    2   0   0    11  13\n"
     ]
    }
   ],
   "source": [
    "## O mesmo de cima só que sem o simple imputer\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === 1. Define transformer to create 'type' column ===\n",
    "def create_type_column(df):\n",
    "    df = df.copy()\n",
    "    df['type'] = df['class'].astype(str) + df['subclass'].astype(str)\n",
    "    return df\n",
    "\n",
    "create_type = FunctionTransformer(create_type_column)\n",
    "\n",
    "# === 2. Select features and target ===\n",
    "FEATURE_COLUMNS = ['effective_temperature_2', 'visual_magnitude', 'mass', 'radius']\n",
    "TARGET_COLUMN = 'type'\n",
    "\n",
    "# === 3. Define the preprocessing pipeline (no imputer now) ===\n",
    "# Only includes transformation to numpy for compatibility\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('features', 'passthrough', FEATURE_COLUMNS)  # no imputation anymore\n",
    "])\n",
    "\n",
    "# === 4. Full pipeline with column transformation and classifier ===\n",
    "model_pipeline = Pipeline([\n",
    "    ('create_type', create_type),  # Add 'type' column\n",
    "    ('preprocessor', preprocessor),  # Extract features\n",
    "    ('to_numpy', FunctionTransformer(lambda df: df.to_numpy(), validate=False))  # Convert to numpy\n",
    "])\n",
    "\n",
    "# === 5. Apply transformations ===\n",
    "# Original DataFrame\n",
    "df1 = df1.dropna(subset=FEATURE_COLUMNS + ['class', 'subclass'])  # Drop rows with missing values\n",
    "\n",
    "# Apply 'type' column creation\n",
    "df_transformed = create_type.transform(df1)\n",
    "X = df_transformed[FEATURE_COLUMNS]\n",
    "y = df_transformed[TARGET_COLUMN]\n",
    "\n",
    "# Encode Labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# === 6. Train/test split ===\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_encoded, test_size=0.3, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "# === 7. Train model ===\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=50,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === 8. Predict and evaluate ===\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(y_val, y_pred),\n",
    "                   index=le.classes_, columns=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3ef196b-4577-4e67-9dee-5740c2d663de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Accuracy: 0.6853\n",
      "\n",
      "Confusion Matrix:\n",
      "     A1  A2  A3  A5  A6    A7  A8  A9    F0  F2\n",
      "A1  186  82   1  69  36   101   0   0     0   0\n",
      "A2  124  62   5  26  23    47   0   1     0   0\n",
      "A3   14   9   1   7   3    39   0   0     0   0\n",
      "A5   84  26   3  99  43   280   1   0     0   0\n",
      "A6   56  32   1  53  54   252   0   0     0   0\n",
      "A7   40  20   3  96  58  1215   2   2     0   0\n",
      "A8    0   2   0   1   0    18   0   1     0   0\n",
      "A9    0   2   0   2   2    18   1   0     0   0\n",
      "F0    0   0   0   0   0     0   0   0  2089   1\n",
      "F2    0   0   0   0   0     0   0   0    19   9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Drop NaNs and create 'type'\n",
    "df1 = df1.dropna(subset=FEATURE_COLUMNS + ['class', 'subclass'])\n",
    "df1['type'] = df1['class'].astype(str) + df1['subclass'].astype(str)\n",
    "\n",
    "# Split by type\n",
    "group_A_types = ['A1', 'A2', 'A3', 'A5', 'A6', 'A7', 'A8', 'A9']\n",
    "group_F_types = ['F0', 'F2']\n",
    "\n",
    "df_A = df1[df1['type'].isin(group_A_types)].copy()\n",
    "df_F = df1[df1['type'].isin(group_F_types)].copy()\n",
    "\n",
    "# Function to train and evaluate a model on a subset\n",
    "def train_subset_model(df_subset):\n",
    "    X = df_subset[FEATURE_COLUMNS]\n",
    "    y = df_subset['type']\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y_encoded, test_size=0.3, stratify=y_encoded, random_state=42\n",
    "    )\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=50,\n",
    "        class_weight='balanced_subsample',\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    return y_val, y_pred, le\n",
    "\n",
    "# Train both models\n",
    "y_val_A, y_pred_A, le_A = train_subset_model(df_A)\n",
    "y_val_F, y_pred_F, le_F = train_subset_model(df_F)\n",
    "\n",
    "# Combine predictions\n",
    "y_val_combined = list(le_A.inverse_transform(y_val_A)) + list(le_F.inverse_transform(y_val_F))\n",
    "y_pred_combined = list(le_A.inverse_transform(y_pred_A)) + list(le_F.inverse_transform(y_pred_F))\n",
    "\n",
    "# Final evaluation\n",
    "print(f\"\\nCombined Accuracy: {accuracy_score(y_val_combined, y_pred_combined):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(y_val_combined, y_pred_combined),\n",
    "                   index=sorted(set(y_val_combined)),\n",
    "                   columns=sorted(set(y_val_combined))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69779bb2-0c15-461a-8aaf-12edba404f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Accuracy: 0.6895\n",
      "\n",
      "Confusion Matrix:\n",
      "     A1  A2  A3   A5  A6    A7  A8  A9    F0  F2\n",
      "A1  201  93   2   72  25    82   0   0     0   0\n",
      "A2  130  65   6   23  20    43   0   1     0   0\n",
      "A3   12  10   1    8   4    38   0   0     0   0\n",
      "A5   80  30   3  104  44   274   1   0     0   0\n",
      "A6   56  32   1   55  59   245   0   0     0   0\n",
      "A7   46  17   2   95  61  1211   2   2     0   0\n",
      "A8    0   1   0    0   0    20   0   1     0   0\n",
      "A9    0   0   0    2   3    19   1   0     0   0\n",
      "F0    0   0   0    0   0     0   0   0  2086   4\n",
      "F2    0   0   0    0   0     0   0   0    17  11\n",
      "CPU times: user 17.5 s, sys: 272 ms, total: 17.7 s\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Drop NaNs and create 'type'\n",
    "df1 = df1.dropna(subset=FEATURE_COLUMNS + ['class', 'subclass'])\n",
    "df1['type'] = df1['class'].astype(str) + df1['subclass'].astype(str)\n",
    "\n",
    "# Split by type\n",
    "group_A_types = ['A1', 'A2', 'A3', 'A5', 'A6', 'A7', 'A8', 'A9']\n",
    "group_F_types = ['F0', 'F2']\n",
    "\n",
    "df_A = df1[df1['type'].isin(group_A_types)].copy()\n",
    "df_F = df1[df1['type'].isin(group_F_types)].copy()\n",
    "\n",
    "# Function to train and evaluate a model on a subset\n",
    "def train_subset_model(df_subset, n):\n",
    "    X = df_subset[FEATURE_COLUMNS]\n",
    "    y = df_subset['type']\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y_encoded, test_size=0.3, stratify=y_encoded, random_state=42\n",
    "    )\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=1000,\n",
    "        max_depth=n + 9,\n",
    "        class_weight='balanced_subsample',\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    return y_val, y_pred, le\n",
    "\n",
    "# Train both models\n",
    "y_val_A, y_pred_A, le_A = train_subset_model(df_A,len(group_A_types))\n",
    "y_val_F, y_pred_F, le_F = train_subset_model(df_F,len(group_F_types))\n",
    "\n",
    "# Combine predictions\n",
    "y_val_combined = list(le_A.inverse_transform(y_val_A)) + list(le_F.inverse_transform(y_val_F))\n",
    "y_pred_combined = list(le_A.inverse_transform(y_pred_A)) + list(le_F.inverse_transform(y_pred_F))\n",
    "\n",
    "# Final evaluation\n",
    "print(f\"\\nCombined Accuracy: {accuracy_score(y_val_combined, y_pred_combined):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(y_val_combined, y_pred_combined),\n",
    "                   index=sorted(set(y_val_combined)),\n",
    "                   columns=sorted(set(y_val_combined))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e10472f-bdeb-4fd6-bc9a-aaaadf1111fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Group A model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:55\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:43\u001b[39m, in \u001b[36mtrain_lgbm_subset\u001b[39m\u001b[34m(df_subset, param_grid, n_classes)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1571\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1570\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/model_selection/_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/joblib/parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/joblib/parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/joblib/parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Compact parameter grids for quick testing\n",
    "param_grid_A = {\n",
    "    'max_depth': [len(group_A_types) + 3, len(group_A_types) + 5],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'num_leaves': [20, 31]\n",
    "}\n",
    "\n",
    "param_grid_F = {\n",
    "    'max_depth': [len(group_F_types) + 2, len(group_F_types) + 4],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [50, 100],\n",
    "    'num_leaves': [10, 15]\n",
    "}\n",
    "\n",
    "def train_lgbm_subset(df_subset, param_grid, n_classes):\n",
    "    X = df_subset[FEATURE_COLUMNS]\n",
    "    y = df_subset['type']\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y_encoded, test_size=0.3, stratify=y_encoded, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fast grid search with small parameter space\n",
    "    lgbm = LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        random_state=42,\n",
    "        verbosity=-1  # Silence output\n",
    "    )\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=lgbm,\n",
    "        param_grid=param_grid,\n",
    "        cv=2,  # Fewer folds for speed\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Validation Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "    \n",
    "    return y_val, y_pred, le, best_model\n",
    "\n",
    "# Train both models\n",
    "print(\"Training Group A model...\")\n",
    "y_val_A, y_pred_A, le_A, model_A = train_lgbm_subset(df_A, param_grid_A, len(group_A_types))\n",
    "\n",
    "print(\"\\nTraining Group F model...\")\n",
    "y_val_F, y_pred_F, le_F, model_F = train_lgbm_subset(df_F, param_grid_F, len(group_F_types))\n",
    "\n",
    "# Combine predictions\n",
    "y_val_combined = list(le_A.inverse_transform(y_val_A)) + list(le_F.inverse_transform(y_val_F))\n",
    "y_pred_combined = list(le_A.inverse_transform(y_pred_A)) + list(le_F.inverse_transform(y_pred_F))\n",
    "\n",
    "# Final evaluation\n",
    "print(f\"\\nCombined Accuracy: {accuracy_score(y_val_combined, y_pred_combined):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(y_val_combined, y_pred_combined),\n",
    "                   index=sorted(set(y_val_combined)),\n",
    "                   columns=sorted(set(y_val_combined))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21e6253d-d4ab-4622-8f08-9bd4ef8e1edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Category Counts for 'type' ===\n",
      "type\n",
      "F0    6966\n",
      "A7    4788\n",
      "A5    1788\n",
      "A1    1582\n",
      "A6    1493\n",
      "A2     960\n",
      "A3     243\n",
      "F2      93\n",
      "A9      83\n",
      "A8      73\n",
      "A0       2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Remaining Classes After Filtering ===\n",
      "type\n",
      "F0    6897\n",
      "A7    2814\n",
      "A5     780\n",
      "A6     672\n",
      "F2      91\n",
      "A3      49\n",
      "A8      46\n",
      "A9      45\n",
      "A2      38\n",
      "A1      34\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Count and display the number of entries per category\n",
    "print(\"\\n=== Category Counts for 'type' ===\")\n",
    "print(df1['type'].value_counts())\n",
    "\n",
    "# 2. Define minimum count threshold (customizable)\n",
    "MIN_COUNT = 1  # Modify this value as needed\n",
    "\n",
    "# 3. Filter out classes with fewer entries than MIN_COUNT\n",
    "valid_classes = df1['type'].dropna().value_counts()\n",
    "valid_classes = valid_classes[valid_classes >= MIN_COUNT].index\n",
    "\n",
    "df_filtered1 = df1[df1['type'].isin(valid_classes)].reset_index(drop=True).dropna()\n",
    "\n",
    "print(\"\\n=== Remaining Classes After Filtering ===\")\n",
    "print(df_filtered1['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c3cb8bb-b01c-447e-84f7-980dc3d93e5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 2. Train Model\u001b[39;00m\n\u001b[32m     28\u001b[39m model = RandomForestClassifier(\n\u001b[32m     29\u001b[39m     n_estimators=\u001b[32m50\u001b[39m,\n\u001b[32m     30\u001b[39m     max_depth=\u001b[32m50\u001b[39m,\n\u001b[32m     31\u001b[39m     class_weight=\u001b[33m'\u001b[39m\u001b[33mbalanced_subsample\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     32\u001b[39m     random_state=\u001b[32m42\u001b[39m\n\u001b[32m     33\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# 3. Evaluation\u001b[39;00m\n\u001b[32m     37\u001b[39m y_pred = model.predict(X_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/ensemble/_forest.py:487\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    476\u001b[39m trees = [\n\u001b[32m    477\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    479\u001b[39m ]\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    486\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    509\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/joblib/parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/joblib/parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/utils/parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/ensemble/_forest.py:189\u001b[39m, in \u001b[36m_parallel_build_trees\u001b[39m\u001b[34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m class_weight == \u001b[33m\"\u001b[39m\u001b[33mbalanced_subsample\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    187\u001b[39m         curr_sample_weight *= compute_sample_weight(\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m, y, indices=indices)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    197\u001b[39m     tree._fit(\n\u001b[32m    198\u001b[39m         X,\n\u001b[32m    199\u001b[39m         y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m         missing_values_in_feature_mask=missing_values_in_feature_mask,\n\u001b[32m    203\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/numen_DS/lib/python3.13/site-packages/sklearn/tree/_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration\n",
    "TARGET_COLUMN = 'type'\n",
    "FEATURE_COLUMNS = ['effective_temperature_2', 'visual_magnitude', 'mass', 'radius']\n",
    "\n",
    "# Prepare feature data\n",
    "X = df_filtered1.dropna()[FEATURE_COLUMNS]\n",
    "\n",
    "# Prepare target data\n",
    "y = df_filtered1.dropna()[TARGET_COLUMN]\n",
    "\n",
    "# Split data (stratified)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Handle Class Imbalance with SMOTE\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 2. Train Model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=50,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# 3. Evaluation\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# # Print Accuracy and Confusion Matrix\n",
    "# print(f\"Validation Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "# print(\"\\nConfusion Matrix (True vs Predicted):\")\n",
    "# print(pd.DataFrame(\n",
    "#     confusion_matrix(y_val, y_pred),\n",
    "#     index=np.unique(y),\n",
    "#     columns=np.unique(y)\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b16df3-2d28-452d-bc7b-0af29e9e1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Accuracy  \n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_pred):.4f}\")  \n",
    "\n",
    "# Confusion Matrix (Absolute Values)  \n",
    "plt.figure(figsize=(8, 6))  \n",
    "sns.heatmap(  \n",
    "    confusion_matrix(y_val, y_pred),  \n",
    "    annot=True,  \n",
    "    fmt=\"d\",  \n",
    "    cmap=\"Blues\",  \n",
    "    xticklabels=np.unique(y),  \n",
    "    yticklabels=np.unique(y),  \n",
    ")  \n",
    "plt.title(\"Confusion Matrix (Absolute Counts)\")  \n",
    "plt.xlabel(\"Predicted\")  \n",
    "plt.ylabel(\"True\")  \n",
    "plt.show()  \n",
    "\n",
    "# Confusion Matrix (Normalized Percentage)  \n",
    "plt.figure(figsize=(8, 6))  \n",
    "sns.heatmap(  \n",
    "    confusion_matrix(y_val, y_pred, normalize=\"true\"),  \n",
    "    annot=True,  \n",
    "    fmt=\".2%\",  \n",
    "    cmap=\"Greens\",  \n",
    "    xticklabels=np.unique(y),  \n",
    "    yticklabels=np.unique(y),  \n",
    ")  \n",
    "plt.title(\"Confusion Matrix (Normalized by True Class %)\")  \n",
    "plt.xlabel(\"Predicted\")  \n",
    "plt.ylabel(\"True\")  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6fd71-f608-4ac5-9931-dd6e3739a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier# Print Accuracy and Confusion Matrix  \n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_pred):.4f}\")  \n",
    "\n",
    "plt.figure(figsize=(8, 6))  \n",
    "sns.heatmap(  \n",
    "    confusion_matrix(y_val, y_pred),  \n",
    "    annot=True,  \n",
    "    fmt=\"d\",  \n",
    "    cmap=\"Blues\",  \n",
    "    xticklabels=np.unique(y),  \n",
    "    yticklabels=np.unique(y)  \n",
    ")  \n",
    "plt.title(\"Confusion Matrix (True vs Predicted)\")  \n",
    "plt.xlabel(\"Predicted\")  \n",
    "plt.ylabel(\"True\")  \n",
    "plt.show()  \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration\n",
    "TARGET_COLUMN = 'luminosity_class'\n",
    "FEATURE_COLUMNS = ['luminosity', 'visual_magnitude', 'mass', 'log_surface_gravity_2']\n",
    "\n",
    "\n",
    "\n",
    "# Prepare feature data\n",
    "X = df_lum[FEATURE_COLUMNS]\n",
    "\n",
    "# Prepare target data\n",
    "y = df_lum[TARGET_COLUMN]\n",
    "\n",
    "# Split data (stratified)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Handle Class Imbalance with SMOTE\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 2. Train Model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=7,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# 3. Evaluation\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Print Accuracy and Confusion Matrix\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "print(\"\\nConfusion Matrix (True vs Predicted):\")\n",
    "print(pd.DataFrame(\n",
    "    confusion_matrix(y_val, y_pred),\n",
    "    index=np.unique(y),\n",
    "    columns=np.unique(y)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18199e-b765-496a-ad4d-d1b0be32b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE  # For handling class imbalance\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    min_samples_split=int(1e3)\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"\\nValidation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "#print(\"\\nClassification Report:\")\n",
    "#print(classification_report(y_val, y_pred, \n",
    "#                          target_names=label_encoder.classes_,\n",
    "#                          zero_division=0))\n",
    "\n",
    "# Predictions\n",
    "#results = df_imputed.iloc[val_indices].copy()\n",
    "#results['Predicted'] = label_encoder.inverse_transform(y_pred)\n",
    "#print(\"\\nSample Predictions:\")\n",
    "#print(results[[TARGET_COLUMN] + FEATURE_COLUMNS + ['Predicted']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97136a-7711-4f04-a87e-bc6ad1a72173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE  # For handling class imbalance\n",
    "\n",
    "# 1. Handle Class Imbalance\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 2. Train Model with Balanced Data and Optimized Parameters\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=1000,  # Increased from 100\n",
    "    max_depth=8,      # Increased from 7\n",
    "    min_samples_split=int(1e2),  # Reduced from 1000\n",
    "    class_weight='balanced_subsample',  # Added class weighting\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all cores\n",
    ")\n",
    "\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# 3. Enhanced Evaluation\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(\"\\n=== Enhanced Evaluation ===\")\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "\n",
    "# Detailed class performance\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, \n",
    "                          target_names=label_encoder.classes_,\n",
    "                          zero_division=0))\n",
    "\n",
    "# Confusion matrix for error analysis\n",
    "print(\"\\nConfusion Matrix (True vs Predicted):\")\n",
    "print(pd.DataFrame(confusion_matrix(y_val, y_pred),\n",
    "      index=label_encoder.classes_,\n",
    "      columns=label_encoder.classes_))\n",
    "\n",
    "# 4. Predictions with Confidence\n",
    "probs = model.predict_proba(X_val)\n",
    "results = df_filtered.iloc[val_indices].copy()\n",
    "results['Predicted'] = label_encoder.inverse_transform(y_pred)\n",
    "results['Confidence'] = np.max(probs, axis=1)  # Add confidence scores\n",
    "\n",
    "print(\"\\nSample Predictions with Confidence Scores:\")\n",
    "print(results[[TARGET_COLUMN] + FEATURE_COLUMNS + ['Predicted', 'Confidence']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea84a89-1974-474d-a087-1063a0ba14ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration\n",
    "TARGET_COLUMN = 'SubClass'  # Changed to string since it's a single column\n",
    "FEATURE_COLUMNS = ['Teff2', 'logg2', 'Rad']\n",
    "\n",
    "# Prepare data - no label encoding needed since subclasses are already numeric\n",
    "y = df_filtered[TARGET_COLUMN].astype(int)  # Ensure integer type\n",
    "X = df_filtered[FEATURE_COLUMNS]\n",
    "\n",
    "# Get class names for reporting\n",
    "class_names = sorted(y.unique())\n",
    "\n",
    "# Split data (stratified)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, \n",
    "                                                 stratify=y, random_state=42)\n",
    "\n",
    "# 1. Handle Class Imbalance automatically with SMOTE\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 2. Train Model with Balanced Data and Optimized Parameters\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,  # Increased for better performance\n",
    "    max_depth=10,      # Increased depth for more complex relationships\n",
    "    min_samples_leaf=5, # Added to prevent overfitting\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42,\n",
    "    n_jobs=-1          # Re-enabled parallel processing\n",
    ")\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# 3. Enhanced Evaluation\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(\"\\n=== Enhanced Evaluation ===\")\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, \n",
    "                          target_names=[str(c) for c in class_names],\n",
    "                          zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix (True vs Predicted):\")\n",
    "print(pd.DataFrame(confusion_matrix(y_val, y_pred),\n",
    "                  index=class_names,\n",
    "                  columns=class_names))\n",
    "\n",
    "# 4. Predictions with Confidence\n",
    "probs = model.predict_proba(X_val)\n",
    "results = df_filtered.iloc[X_val.index].copy()\n",
    "results['Predicted'] = y_pred\n",
    "results['Confidence'] = np.max(probs, axis=1)\n",
    "\n",
    "print(\"\\nSample Predictions with Confidence Scores:\")\n",
    "print(results[[TARGET_COLUMN] + FEATURE_COLUMNS + ['Predicted', 'Confidence']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9488b-f4e0-4e9c-87ff-a423391bff02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be3146c-a30d-443b-9577-15698d62a7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df32059-47e6-4dbb-9025-1a221cdf40f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.impute import KNNImputer\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# def knn_impute_with_categorical(df, n_neighbors=5):\n",
    "#     \"\"\"\n",
    "#     Applies KNN imputation while preserving categorical columns.\n",
    "    \n",
    "#     Parameters:\n",
    "#     df (pd.DataFrame): Input dataframe with mixed data types\n",
    "#     n_neighbors (int): Number of neighbors for KNN\n",
    "    \n",
    "#     Returns:\n",
    "#     pd.DataFrame: Imputed dataframe with original dtypes preserved\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Create copy to avoid modifying original\n",
    "#     df_imputed = df.copy()\n",
    "    \n",
    "#     # Identify categorical and numerical columns\n",
    "#     categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "#     numerical_cols = df.select_dtypes(include=np.number).columns\n",
    "    \n",
    "#     # Label encode categorical columns\n",
    "#     encoders = {}\n",
    "#     for col in categorical_cols:\n",
    "#         le = LabelEncoder()\n",
    "#         df_imputed[col] = le.fit_transform(df[col].astype(str))\n",
    "#         encoders[col] = le\n",
    "    \n",
    "#     # Apply KNN imputation to all columns (now numerical)\n",
    "#     imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "#     imputed_data = imputer.fit_transform(df_imputed)\n",
    "#     df_imputed = pd.DataFrame(imputed_data, columns=df.columns)\n",
    "    \n",
    "#     # Convert categorical columns back to original labels\n",
    "#     for col in categorical_cols:\n",
    "#         df_imputed[col] = encoders[col].inverse_transform(df_imputed[col].round().astype(int))\n",
    "    \n",
    "#     # Restore original dtypes\n",
    "#     for col, dtype in df.dtypes.items():\n",
    "#         if col in categorical_cols:\n",
    "#             df_imputed[col] = df_imputed[col].astype(dtype)\n",
    "#         else:\n",
    "#             df_imputed[col] = df_imputed[col].astype(dtype) if dtype != object else df_imputed[col]\n",
    "    \n",
    "#     return df_imputed\n",
    "\n",
    "# # Usage example:\n",
    "# df_imputed = knn_impute_with_categorical(df1, n_neighbors=5)\n",
    "df_imputed = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63253bfd-7ebd-4d5f-a4f9-547518d755d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove categories with 10 or fewer elements\n",
    "value_counts = df_imputed['Ca_K'].value_counts()  # Replace 'Ca_K' with your target column\n",
    "valid_categories = value_counts[value_counts > 1000].index\n",
    "df_filtered = df_imputed[df_imputed['Ca_K'].isin(valid_categories)].copy()  # Replace 'Ca_K'\n",
    "\n",
    "# Pre-removal\n",
    "print(\"Categories:\")\n",
    "print(df_imputed['Ca_K'].value_counts())  # Replace 'Ca_K'\n",
    "\n",
    "# Verify removal\n",
    "print(\"Remaining categories:\")\n",
    "print(df_filtered['Ca_K'].value_counts())  # Replace 'Ca_K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82a5a8-7c2c-4208-a2ed-57614801b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999bbaa4-3642-4f83-a42f-ed45f6336353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Configuration\n",
    "TARGET_COLUMN = 'Ca_K'\n",
    "FEATURE_COLUMNS = ['Teff2', 'logg1', 'FeH1', 'Vmag', 'Rad', 'Miss']\n",
    "MAX_SAMPLES_PER_CLASS = int(1e4)  # Customizable\n",
    "\n",
    "# Prepare balanced training set\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_filtered[TARGET_COLUMN])\n",
    "X = df_filtered[FEATURE_COLUMNS]\n",
    "\n",
    "# Stratified sampling with class balancing\n",
    "balanced_samples = []\n",
    "for class_id in np.unique(y):\n",
    "    class_indices = np.where(y == class_id)[0]\n",
    "    n_samples = min(MAX_SAMPLES_PER_CLASS, len(class_indices))\n",
    "    \n",
    "    if len(class_indices) < MAX_SAMPLES_PER_CLASS:\n",
    "        # Sample WITH replacement for small classes\n",
    "        sampled_indices = np.random.choice(class_indices, \n",
    "                                         size=MAX_SAMPLES_PER_CLASS, \n",
    "                                         replace=True)\n",
    "    else:\n",
    "        # Sample WITHOUT replacement for large classes\n",
    "        sampled_indices = np.random.choice(class_indices, \n",
    "                                         size=MAX_SAMPLES_PER_CLASS, \n",
    "                                         replace=False)\n",
    "    \n",
    "    balanced_samples.extend(sampled_indices)\n",
    "\n",
    "X_train = X.iloc[balanced_samples]\n",
    "y_train = y[balanced_samples]\n",
    "\n",
    "# Remaining data for validation\n",
    "val_indices = list(set(range(len(df_filtered))) - set(balanced_samples))\n",
    "X_val = X.iloc[val_indices]\n",
    "y_val = y[val_indices]\n",
    "\n",
    "# Verify counts\n",
    "print(\"Training set class counts:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(\"\\nOriginal class distribution:\")\n",
    "print(pd.Series(y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de63b92-ca0c-4858-adb5-01431fcd1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Configuration\n",
    "TARGET_COLUMN = 'Ca_K'\n",
    "FEATURE_COLUMNS = ['Teff2', 'logg2', 'Vmag', 'Rad', 'Miss']\n",
    "MAX_SAMPLES_PER_CLASS = 2100  # Customizable\n",
    "\n",
    "# Prepare balanced training set\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_filtered[TARGET_COLUMN])\n",
    "X = df_filtered[FEATURE_COLUMNS]\n",
    "\n",
    "# Stratified sampling with equal class sizes\n",
    "balanced_samples = []\n",
    "for class_id in np.unique(y):\n",
    "    class_indices = np.where(y == class_id)[0]\n",
    "    sampled_indices = np.random.choice(class_indices, \n",
    "                                     size=min(MAX_SAMPLES_PER_CLASS, len(class_indices)), \n",
    "                                     replace=False)\n",
    "    balanced_samples.extend(sampled_indices)\n",
    "\n",
    "X_train = X.iloc[balanced_samples]\n",
    "y_train = y[balanced_samples]\n",
    "\n",
    "# Remaining data for validation\n",
    "val_indices = list(set(range(len(df_filtered))) - set(balanced_samples))\n",
    "X_val = X.iloc[val_indices]\n",
    "y_val = y[val_indices]\n",
    "\n",
    "# Verify counts\n",
    "print(\"Training set class counts:\")\n",
    "print(pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99efecd-4edf-4d8a-8032-2ea91e70232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE  # For handling class imbalance\n",
    "\n",
    "# 1. Handle Class Imbalance\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 2. Train Model with Balanced Data and Optimized Parameters\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=1000,  # Increased from 100\n",
    "    max_depth=8,      # Increased from 7\n",
    "    min_samples_split=int(1e2),  # Reduced from 1000\n",
    "    class_weight='balanced_subsample',  # Added class weighting\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all cores\n",
    ")\n",
    "\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# 3. Enhanced Evaluation\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(\"\\n=== Enhanced Evaluation ===\")\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "\n",
    "# Detailed class performance\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, \n",
    "                          target_names=label_encoder.classes_,\n",
    "                          zero_division=0))\n",
    "\n",
    "# Confusion matrix for error analysis\n",
    "print(\"\\nConfusion Matrix (True vs Predicted):\")\n",
    "print(pd.DataFrame(confusion_matrix(y_val, y_pred),\n",
    "      index=label_encoder.classes_,\n",
    "      columns=label_encoder.classes_))\n",
    "\n",
    "# 4. Predictions with Confidence\n",
    "probs = model.predict_proba(X_val)\n",
    "results = df_filtered.iloc[val_indices].copy()\n",
    "results['Predicted'] = label_encoder.inverse_transform(y_pred)\n",
    "results['Confidence'] = np.max(probs, axis=1)  # Add confidence scores\n",
    "\n",
    "print(\"\\nSample Predictions with Confidence Scores:\")\n",
    "print(results[[TARGET_COLUMN] + FEATURE_COLUMNS + ['Predicted', 'Confidence']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f945b-de0e-40ed-b297-2fb53047c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    min_samples_split=int(1e3)\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"\\nValidation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, \n",
    "                          target_names=label_encoder.classes_,\n",
    "                          zero_division=0))\n",
    "\n",
    "# Predictions\n",
    "results = df_filtered.iloc[val_indices].copy()\n",
    "results['Predicted'] = label_encoder.inverse_transform(y_pred)\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(results[[TARGET_COLUMN] + FEATURE_COLUMNS + ['Predicted']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01986f9-dd8f-4792-9dc9-db46319b4995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# XGBoost hyperparameter tuning\n",
    "params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.5,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': len(np.unique(y_train)),\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'early_stopping_rounds': 10,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Train with validation monitoring\n",
    "model = XGBClassifier(**params)\n",
    "eval_set = [(X_train, y_train), (X_val, y_val)]\n",
    "model.fit(X_train, y_train,\n",
    "          eval_set=eval_set,\n",
    "          verbose=True)\n",
    "\n",
    "# Retrieve and print training history\n",
    "results = model.evals_result()\n",
    "epochs = len(results['validation_0']['mlogloss'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x_axis, results['validation_0']['mlogloss'], label='Train')\n",
    "plt.plot(x_axis, results['validation_1']['mlogloss'], label='Validation')\n",
    "plt.legend()\n",
    "plt.ylabel('Log Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('XGBoost Training Progress')\n",
    "plt.show()\n",
    "\n",
    "# Best iteration\n",
    "best_iter = model.best_iteration\n",
    "print(f\"\\nBest iteration: {best_iter}\")\n",
    "print(f\"Best params: {model.get_params()}\")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"\\nValidation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred,\n",
    "                          target_names=label_encoder.classes_,\n",
    "                          zero_division=0))\n",
    "\n",
    "# Feature importance\n",
    "plt.figure(figsize=(10, 5))\n",
    "xgb.plot_importance(model)\n",
    "plt.show()\n",
    "\n",
    "# Predictions\n",
    "results = df_filtered.iloc[val_indices].copy()\n",
    "results['Predicted'] = label_encoder.inverse_transform(y_pred)\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(results[[TARGET_COLUMN] + FEATURE_COLUMNS + ['Predicted']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b6cbe0-ca0b-4421-a4d8-27f7dc12f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#model = SVC()\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Customizable configuration - CHANGE HERE TO MODIFY FEATURES\n",
    "TARGET_COLUMN = 'Ca_K'  # Column to predict\n",
    "FEATURE_COLUMNS = ['Teff2', 'logg1', 'FeH1', 'Vmag', 'Rad','Miss']  # Features to use (modify as needed)\n",
    "\n",
    "class SimpleStellarClassifier:\n",
    "    def __init__(self, target_col, feature_cols):\n",
    "        self.target_col = target_col\n",
    "        self.feature_cols = feature_cols\n",
    "        self.model = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def train(self, df):\n",
    "        \"\"\"Train model with 70/30 random split\"\"\"\n",
    "        # Prepare data\n",
    "        X = df_filtered[self.feature_cols]\n",
    "        y = self.label_encoder.fit_transform(df_filtered[self.target_col])\n",
    "        \n",
    "        # Split data (70% train, 30% validation)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.4, random_state=30\n",
    "        )\n",
    "        \n",
    "        # Train Random Forest (simpler than XGBoost)\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=500,  # 100 trees\n",
    "            max_depth=10,      # Limit tree depth\n",
    "            random_state=42,\n",
    "            min_samples_split = 100,\n",
    "            #class_weight = 'balanced_subsample'\n",
    "        )\n",
    "        # self.model = SVC(\n",
    "        #     kernel='rbf',          # radial basis function kernel (handles non-linearity)\n",
    "        #     C=1.0,                 # regularization (lower = simpler model)\n",
    "        #     gamma='scale',         # auto setting for RBF kernel\n",
    "        #     class_weight='balanced',  # handle class imbalance\n",
    "        #     probability=True,      # enables probability estimates\n",
    "        #     random_state=42\n",
    "        # )\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        # Validate\n",
    "        y_pred = self.model.predict(X_val)\n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_val, y_pred, \n",
    "                                 target_names=self.label_encoder.classes_,\n",
    "                                 zero_division=0))\n",
    "    \n",
    "    def predict(self, df):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        pred_encoded = self.model.predict(df[self.feature_cols])\n",
    "        return self.label_encoder.inverse_transform(pred_encoded)\n",
    "\n",
    "# Usage:\n",
    "# 1. Initialize with your target and features\n",
    "classifier = SimpleStellarClassifier(\n",
    "    target_col=TARGET_COLUMN,\n",
    "    feature_cols=FEATURE_COLUMNS\n",
    ")\n",
    "\n",
    "# 2. Train and validate\n",
    "classifier.train(df_filtered)\n",
    "\n",
    "# 3. Make predictions\n",
    "predictions = classifier.predict(df_filtered)\n",
    "results = df_filtered[[TARGET_COLUMN] + FEATURE_COLUMNS].copy()\n",
    "results['Predicted'] = predictions\n",
    "\n",
    "# Show first 10 predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4264806-06c9-45f0-85f5-51ca9c64a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "\n",
    "# Configuration - MODIFY HERE\n",
    "TARGET_COLUMN = 'Ca_K'  # Column to predict\n",
    "#FEATURE_COLUMNS = ['Teff1','FeH1','Vmag','logg1','Lum','Miss']  # Features to use\n",
    "#FEATURE_COLUMNS = ['Teff1','Miss']  # Features to use\n",
    "\n",
    "class OptimizedStellarClassifier:\n",
    "    def __init__(self, target_col, feature_cols):\n",
    "        self.target_col = target_col\n",
    "        self.feature_cols = feature_cols\n",
    "        self.model = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.best_params = {}\n",
    "        \n",
    "    def _calculate_optimal_depth(self, y):\n",
    "        \"\"\"Determine max_depth based on target cardinality\"\"\"\n",
    "        n_classes = len(np.unique(y))\n",
    "        base_depth = [4, 10, 15, 20, 25, 30]  # Custom depth array - modify values as needed\n",
    "        return {\n",
    "            'max_depth': base_depth,\n",
    "            'n_estimators': [10,100,500]\n",
    "        }\n",
    "        \n",
    "    def train(self, df):\n",
    "        \"\"\"Train model with optimized max_depth\"\"\"\n",
    "        # Prepare data\n",
    "        X = df[self.feature_cols]\n",
    "        y = self.label_encoder.fit_transform(df[self.target_col])\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Determine depth range based on target classes\n",
    "        param_grid = self._calculate_optimal_depth(y)\n",
    "        print(f\"Testing parameters: {param_grid}\")\n",
    "        \n",
    "        # Grid search for best parameters\n",
    "        grid = GridSearchCV(\n",
    "            RandomForestClassifier(\n",
    "                class_weight='balanced_subsample',\n",
    "                random_state=42\n",
    "            ),\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='accuracy'\n",
    "        )\n",
    "        grid.fit(X_train, y_train)\n",
    "        \n",
    "        self.model = grid.best_estimator_\n",
    "        self.best_params = grid.best_params_\n",
    "        \n",
    "        # Validation\n",
    "        y_pred = self.model.predict(X_val)\n",
    "        print(\"\\n=== Best Parameters ===\")\n",
    "        print(f\"max_depth: {self.best_params['max_depth']}\")\n",
    "        print(f\"n_estimators: {self.best_params['n_estimators']}\")\n",
    "        \n",
    "        print(\"\\n=== Validation Metrics ===\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_val, y_pred,\n",
    "                                 target_names=self.label_encoder.classes_,\n",
    "                                 zero_division=0))\n",
    "    \n",
    "    def predict(self, df):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        pred_encoded = self.model.predict(df[self.feature_cols])\n",
    "        return self.label_encoder.inverse_transform(pred_encoded)\n",
    "\n",
    "# Usage\n",
    "classifier = OptimizedStellarClassifier(\n",
    "    target_col=TARGET_COLUMN,\n",
    "    feature_cols=FEATURE_COLUMNS\n",
    ")\n",
    "\n",
    "# Train with automatic depth tuning\n",
    "classifier.train(df_filtered)\n",
    "\n",
    "# Predictions\n",
    "predictions = classifier.predict(df_filtered)\n",
    "results = df_filtered[[TARGET_COLUMN] + FEATURE_COLUMNS].copy()\n",
    "results['Predicted'] = predictions\n",
    "\n",
    "print(\"\\n=== First 10 Predictions ===\")\n",
    "print(results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a65ba6-5db6-416d-b7c4-4ae939c6d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: classes (3000 cada), com depth = 20 deu 0.74 o melhor\n",
    "# 3: classes, maxdepth = 10, 500 estimadores, max deu 0.7529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db09dc8-1f14-4d4b-9146-821e78493f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f51be-b7aa-40d6-ab8f-7f64d8652e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ed81f-3ba3-4ded-8cb0-4a5aa8666c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8da27-3b6a-40e6-9ba8-7537242a4ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
